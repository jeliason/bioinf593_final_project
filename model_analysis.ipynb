{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_data = pd.read_excel('./CRC_TMAs_patient_annotations.xlsx')\n",
    "cell_data = pd.read_csv('./CRC_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = \"label\"\n",
    "CENTROID = \"centroid\"\n",
    "FEATURES = \"feat\"\n",
    "def data_to_dgl_graph(pt_data, cell_data, k=5, thresh=50, mode=\"distance\", normalize=False):\n",
    "    labs, ids = np.unique(cell_data.loc[:,'ClusterName'].to_numpy(), return_inverse=True)\n",
    "    cell_data['ClusterID'] = ids\n",
    "    graphs = []\n",
    "    patients = []\n",
    "    targets = []\n",
    "    for spot in cell_data[\"spots\"].unique():\n",
    "        subset = cell_data.loc[cell_data.loc[:,'spots'] == spot,:]\n",
    "        features = subset.loc[:,'size':'Treg-PD-1+'].to_numpy() # TODO: This step depends on column order\n",
    "        if normalize:\n",
    "            features = np.nan_to_num((features-features.mean(axis=0))/features.std(axis=0))\n",
    "        centroids = subset.loc[:,'X':'Y'].to_numpy()\n",
    "        annotation = subset.loc[:,'ClusterID'].to_numpy()\n",
    "        num_nodes = features.shape[0]\n",
    "        graph = dgl.DGLGraph()\n",
    "        graph.add_nodes(num_nodes)\n",
    "        graph.ndata[CENTROID] = torch.FloatTensor(centroids)\n",
    "        graph.ndata[FEATURES] = torch.FloatTensor(features)\n",
    "        if annotation is not None:\n",
    "            graph.ndata[LABEL] = torch.FloatTensor(annotation.astype(float))\n",
    "        adj = kneighbors_graph(\n",
    "            centroids,\n",
    "            k,\n",
    "            mode=mode,\n",
    "            include_self=False,\n",
    "            metric=\"euclidean\").toarray()\n",
    "        if thresh is not None:\n",
    "            adj[adj > thresh] = 0\n",
    "        edge_list = np.nonzero(adj)\n",
    "        graph.add_edges(list(edge_list[0]), list(edge_list[1]))\n",
    "        graphs.append(graph)\n",
    "        assert(len(subset['patients'].unique()) == 1)\n",
    "        patients.append(subset['patients'].unique()[0])\n",
    "    for pt in patients:\n",
    "        cp = pt_data.loc[pt_data.loc[:,'Patient'] == pt,'cp_TNM_Simple'].values[0]\n",
    "        assert(cp == 3.0 or cp == 4.0)\n",
    "        t = 0 if cp == 3.0 else 1\n",
    "        targets.append(t)\n",
    "    return list(zip(graphs,targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_distance = data_to_dgl_graph(pt_data, cell_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_connectivity = data_to_dgl_graph(pt_data, cell_data, mode=\"connectivity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import trange\n",
    "from histocartography.ml import CellGraphModel\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate(batch):\n",
    "    g = dgl.batch([example[0] for example in batch])\n",
    "    l = torch.LongTensor([example[1] for example in batch])\n",
    "    return g, l\n",
    "\n",
    "def dataset_split(data, val_prop):\n",
    "    if val_prop == 0:\n",
    "        return data, data\n",
    "    random.shuffle(data)\n",
    "    train_data = data[:int(len(data)*val_prop)]\n",
    "    val_data = data[int(len(data)*val_prop):]\n",
    "    return train_data, val_data\n",
    "\n",
    "#TODO: More sophisticated oversample\n",
    "def oversample_positive(data, oversample_factor=2):\n",
    "    negative = []\n",
    "    positive = []\n",
    "    for item in data:\n",
    "        if item[1] == 0:\n",
    "            negative.append(item)\n",
    "        else:\n",
    "            positive.append(item)\n",
    "    positive = oversample_factor*positive\n",
    "    return positive+negative\n",
    "\n",
    "class CGModel():\n",
    "    def __init__(self, gnn_params, classification_params, node_dim, num_classes=2, lr=10e-3, weight_decay=5e-4, num_epochs=50, batch_size=8):\n",
    "        self.gnn_params = gnn_params\n",
    "        self.classification_params = classification_params\n",
    "        self.node_dim = node_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.cgm = CellGraphModel(gnn_params, classification_params, node_dim, num_classes=2)\n",
    "\n",
    "    def train(self, data, val_prop=0, oversample_factor=1):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.cgm.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        # define loss function\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        # training\n",
    "        loss = 10e5\n",
    "        val_accuracy = 0.\n",
    "        train_dataloader = None\n",
    "        val_dataloader = None\n",
    "        loss_list = []\n",
    "        val_accuracy_list = []\n",
    "        train_data, val_data = dataset_split(data, val_prop)\n",
    "        train_data = oversample_positive(train_data, oversample_factor=oversample_factor)\n",
    "        train_dataloader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True, collate_fn=collate)\n",
    "        val_dataloader = DataLoader(val_data, batch_size=self.batch_size, shuffle=True, collate_fn=collate)\n",
    "        with trange(self.num_epochs) as t:\n",
    "            for epoch in t:\n",
    "                t.set_description('Loss={} | Val Accuracy={}'.format(loss, val_accuracy))\n",
    "                self.cgm.train()\n",
    "                for graphs, labels in train_dataloader:\n",
    "                    logits = self.cgm(graphs)\n",
    "                    loss = loss_fn(logits, labels)\n",
    "                    loss_list.append(loss.item())\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                self.cgm.eval()\n",
    "                all_val_logits = []\n",
    "                all_val_labels = []\n",
    "                for graphs, labels in val_dataloader:\n",
    "                    with torch.no_grad():\n",
    "                        logits = self.cgm(graphs)\n",
    "                    all_val_logits.append(logits)\n",
    "                    all_val_labels.append(labels)\n",
    "                all_val_logits = torch.cat(all_val_logits).cpu()\n",
    "                all_val_labels = torch.cat(all_val_labels).cpu()\n",
    "                with torch.no_grad():\n",
    "                    _, predictions = torch.max(all_val_logits, dim=1)\n",
    "                    correct = torch.sum(predictions.to(int) == all_val_labels.to(int))\n",
    "                    val_accuracy = round(correct.item() * 1.0 / len(all_val_labels), 2)\n",
    "                    val_accuracy_list.append(val_accuracy)\n",
    "        return loss_list, val_accuracy_list\n",
    "    \n",
    "    def infer(self, graphs):\n",
    "        with torch.no_grad():\n",
    "            logits = self.cgm(graphs)\n",
    "            _, predictions = torch.max(logits, dim=1)\n",
    "            return predictions\n",
    "    \n",
    "    def test(self, graphs, labels):\n",
    "        predictions = self.infer(graphs)\n",
    "        with torch.no_grad():\n",
    "            correct = torch.sum(predictions.to(int) == labels.to(int))\n",
    "            accuracy = round(correct.item() * 1.0 / len(labels), 2)\n",
    "            return accuracy\n",
    "    \n",
    "    def save(self, model_path):\n",
    "        #TODO: Save model and params to model path\n",
    "        pass\n",
    "\n",
    "    def load(self, model_path):\n",
    "        #TODO: Load model cpt and params from model path\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_params = {\n",
    "    'readout_op': 'concat',\n",
    "    'layer_type': 'gin_layer',\n",
    "    'output_dim': 32,\n",
    "    'num_layers': 2,\n",
    "    'readout_type': 'mean'\n",
    "}\n",
    "classification_params = {\n",
    "    'hidden_dim': 20,\n",
    "    'num_layers': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = cell_data.loc[:,'size':'Treg-PD-1+'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = features.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = features.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = (features-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.85247023, -0.81969919, -0.53165024, ..., -0.06755994,\n",
       "        -0.04130121, -0.04153562],\n",
       "       [-0.97254549, -0.73549292, -0.53007786, ..., -0.06755994,\n",
       "        -0.04130121, -0.04153562],\n",
       "       [ 0.08099259, -0.41216812, -0.15772312, ..., -0.06755994,\n",
       "        -0.04130121, -0.04153562],\n",
       "       ...,\n",
       "       [-0.8197874 ,  0.33839227,  0.01888798, ..., -0.06755994,\n",
       "        -0.04130121, -0.04153562],\n",
       "       [-0.46519151, -0.39686416, -0.24040344, ..., -0.06755994,\n",
       "        -0.04130121, -0.04153562],\n",
       "       [ 0.24695686, -0.05728709, -0.55965065, ..., -0.06755994,\n",
       "        -0.04130121, -0.04153562]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = cell_data.loc[cell_data.loc[:,'spots'] == \"1_A\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = subset.loc[:,'size':'Treg-PD-1+'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.80316299e+03, 7.11740685e+01, 1.02455552e+02, 3.49946084e+02,\n",
       "       5.65603834e+02, 2.62496114e+01, 1.48557536e+02, 1.15134739e+02,\n",
       "       1.76485240e+02, 1.76445713e+01, 1.06407223e+02, 8.14510483e+00,\n",
       "       1.27827952e+02, 2.39994481e+02, 1.15089871e+01, 8.77918635e+00,\n",
       "       1.77025247e+02, 3.22486178e+01, 3.62872757e+01, 8.87892531e+01,\n",
       "       1.57795074e+01, 5.85449549e+00, 1.40188555e+02, 4.11823975e+01,\n",
       "       3.11412825e+00, 4.09298862e+02, 1.18326770e+02, 3.72497793e+01,\n",
       "       1.57028497e+02, 2.65340507e+01, 3.58463394e+01, 4.81728326e+01,\n",
       "       1.01512201e+01, 1.41964474e+02, 6.57819254e+01, 1.96262912e+01,\n",
       "       5.53325022e+00, 2.84098839e+02, 2.76688754e+01, 4.35980413e+01,\n",
       "       2.32691978e+02, 6.82050953e+00, 1.91373544e+03, 3.01795052e+02,\n",
       "       3.49912562e+02, 3.62520359e+01, 2.54792174e+01, 5.81637926e+02,\n",
       "       2.06596242e+02, 2.53188475e+00, 7.41416515e+02, 9.75567688e+01,\n",
       "       9.19289297e+00, 5.71194626e+02, 7.97517047e+00, 6.63206233e+02,\n",
       "       3.82924890e+01, 2.39832568e+01, 1.77538262e+03, 4.14157072e-02,\n",
       "       0.00000000e+00, 5.07018443e-02, 0.00000000e+00, 2.92979260e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.92979260e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.92979260e-02, 0.00000000e+00, 2.92979260e-02,\n",
       "       0.00000000e+00, 0.00000000e+00])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-7ecfc1e928f2>:1: RuntimeWarning: invalid value encountered in true_divide\n",
      "  norm = (features-features.mean(axis=0))/features.std(axis=0)\n"
     ]
    }
   ],
   "source": [
    "norm = (features-features.mean(axis=0))/features.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.81810758, -1.09596514, -0.78998727, ..., -0.02932312,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.48494687, -0.69636828, -0.78045111, ..., -0.02932312,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.42182773,  0.83795437,  1.47780854, ..., -0.02932312,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.33335625, -0.74788513, -0.18652341, ..., -0.02932312,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.56235942, -0.4196701 , -0.18126653, ..., -0.02932312,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.67265019,  0.0446426 , -0.49388344, ..., -0.02932312,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nan_to_num(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f32023d3187841795697d47a9066836f1d96daf3799e179b4995fcdf98db168e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
